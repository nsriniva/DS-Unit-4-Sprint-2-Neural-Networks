{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NGGrt9EYlCqY"
   },
   "source": [
    "\n",
    "\n",
    "# Train Practice\n",
    "\n",
    "## *Data Science Unit 4 Sprint 2 Assignment 2*\n",
    "\n",
    "Continue to use TensorFlow Keras & a sample of the [Quickdraw dataset](https://github.com/googlecreativelab/quickdraw-dataset) to build a sketch classification model. The dataset has been sampled to only 10 classes and 10000 observations per class. Please build a baseline classification model then run a few experiments with different optimizers and learning rates. \n",
    "\n",
    "*Don't forgot to switch to GPU on Colab!*\n",
    "\n",
    "\n",
    "------\n",
    "\n",
    "# Objective \n",
    "\n",
    "We are going to run a few experiments today\n",
    "\n",
    "- Train a model with and without normalized data and investigate the weight values and learning outcomes\n",
    "- Train a model with varying values for batch_size, learning_rate, and optimizers\n",
    "\n",
    "We are essentially running mannual gridsearches on our models. In module 3, we'll learn a few different ways to automate gridseach for deep learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, ReLU, Input\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from keras.optimizers import Adam, SGD, Adamax, Adagrad, Adadelta\n",
    "from keras.activations import relu\n",
    "from tensorflow.keras.callbacks import TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data\n",
    "\n",
    "- Don't normalize your data just yet!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nJsIsrvp7O3e"
   },
   "outputs": [],
   "source": [
    "def load_quickdraw10():\n",
    "    \"\"\"\n",
    "    Fill out this doc string, and comment the code, for practice in writing the kind of code that will get you hired. \n",
    "    \"\"\"\n",
    "    \n",
    "    URL_ = \"https://github.com/LambdaSchool/DS-Unit-4-Sprint-2-Neural-Networks/blob/main/quickdraw10.npz?raw=true\"\n",
    "    \n",
    "    path_to_zip = tf.keras.utils.get_file('./quickdraw10.npz', origin=URL_, extract=False)\n",
    "\n",
    "    data = np.load(path_to_zip)\n",
    "\n",
    "    X = data['arr_0']\n",
    "    Y = data['arr_1']\n",
    "        \n",
    "    return train_test_split(X, Y, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = load_quickdraw10()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_labels = len(np.unique(y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "l-6PxI6H5__2"
   },
   "source": [
    "----\n",
    "### Write a Model Function\n",
    "- Write a function called `create_model` which returns a compiled TensorFlow Keras Sequential Model suitable for classifying the QuickDraw-10 dataset. \n",
    "\n",
    "Your function `create_model` should accept the following parameters\n",
    "\n",
    "- Learning Rate `lr`\n",
    "- Optimizer `opt`\n",
    "\n",
    "\n",
    "Build a model with the following architecture and parameter values\n",
    "\n",
    "- Use `1 hidden layer` \n",
    "- Use `sigmoid` activation function in hidden layer\n",
    "- Use `250 nodes` in hidden layer \n",
    "- Use `10 nodes` in output layer\n",
    "- Use `softmax` activation fucntion in output layer\n",
    "- Use `sparse_categorical_crossentropy` loss function\n",
    "- Use `accuracy` as your metric \n",
    "\n",
    "We will use this function to build all the models that we'll need to run our experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "deletable": false,
    "id": "nEREYT-3wI1f",
    "nbgrader": {
     "cell_type": "code",
     "checksum": "6692fdabca8804cd50380fd4b9a56169",
     "grade": false,
     "grade_id": "cell-355125ca910bfedb",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "def create_model(lr=.01, opt=\"adam\"):\n",
    "    \"\"\"\n",
    "    \n",
    "    Build and returns a complies Keras model.  \n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    lr: float\n",
    "        Learing rate parameter used for Stocastic Gradient Descent \n",
    "        \n",
    "    opt: string\n",
    "        Name of optimizer to use\n",
    "        Valid options are \"adam\", \"sgd\", \"adamax\", \"adagrad\", \"adadelta\" \n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    model: keras object \n",
    "        A complied keras model \n",
    "    \"\"\"\n",
    "    optimizers = {'adam':Adam, 'sgd':SGD, 'adamax':Adamax, 'adagrad':Adagrad, 'adadelta':Adadelta}\n",
    "    \n",
    "    # Instantiate optimizer specified by opt \n",
    "    # If opt is not one of the valid options, defaults to Adam\n",
    "    opt = optimizers.get(opt,Adam)(learning_rate=lr)\n",
    "    \n",
    "\n",
    "    # build model here\n",
    "# YOUR CODE HERE\n",
    "    model = Sequential([\n",
    "    Dense(250, input_dim=784, activation='sigmoid'),\n",
    "    Dense(10, activation='softmax')\n",
    "  ])\n",
    "    model.compile(optimizer=opt, \n",
    "                loss='sparse_categorical_crossentropy',\n",
    "                metrics=['accuracy'])\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   <tensorflow.python.keras.layers.core.Dense object at 0x7fbe52a5db00>,\n",
      "    <tensorflow.python.keras.layers.core.Dense object at 0x7fbe41a662b0>]\n",
      "[   {   'class_name': 'InputLayer',\n",
      "        'config': {   'batch_input_shape': (None, 784),\n",
      "                      'dtype': 'float32',\n",
      "                      'name': 'dense_input',\n",
      "                      'ragged': False,\n",
      "                      'sparse': False}},\n",
      "    {   'class_name': 'Dense',\n",
      "        'config': {   'activation': 'sigmoid',\n",
      "                      'activity_regularizer': None,\n",
      "                      'batch_input_shape': (None, 784),\n",
      "                      'bias_constraint': None,\n",
      "                      'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
      "                      'bias_regularizer': None,\n",
      "                      'dtype': 'float32',\n",
      "                      'kernel_constraint': None,\n",
      "                      'kernel_initializer': {   'class_name': 'GlorotUniform',\n",
      "                                                'config': {'seed': None}},\n",
      "                      'kernel_regularizer': None,\n",
      "                      'name': 'dense',\n",
      "                      'trainable': True,\n",
      "                      'units': 250,\n",
      "                      'use_bias': True}},\n",
      "    {   'class_name': 'Dense',\n",
      "        'config': {   'activation': 'softmax',\n",
      "                      'activity_regularizer': None,\n",
      "                      'bias_constraint': None,\n",
      "                      'bias_initializer': {'class_name': 'Zeros', 'config': {}},\n",
      "                      'bias_regularizer': None,\n",
      "                      'dtype': 'float32',\n",
      "                      'kernel_constraint': None,\n",
      "                      'kernel_initializer': {   'class_name': 'GlorotUniform',\n",
      "                                                'config': {'seed': None}},\n",
      "                      'kernel_regularizer': None,\n",
      "                      'name': 'dense_1',\n",
      "                      'trainable': True,\n",
      "                      'units': 10,\n",
      "                      'use_bias': True}}]\n",
      "3 softmax\n"
     ]
    }
   ],
   "source": [
    "from pprint import PrettyPrinter\n",
    "\n",
    "pp = PrettyPrinter(indent=4)\n",
    "# a check on model architecture\n",
    "model = create_model()\n",
    "\n",
    "pp.pprint(model.layers)\n",
    "pp.pprint(model.get_config()['layers'])\n",
    "\n",
    "n_layers = len(model.get_config()[\"layers\"])\n",
    "output_act_funct =  model.get_config()[\"layers\"][-1][\"config\"][\"activation\"]\n",
    "print(n_layers, output_act_funct)\n",
    "\n",
    "assert n_layers == 3, \"You should have an input, one hidden, and an output layer\"\n",
    "assert output_act_funct == \"softmax\", \"Output act funct should be softmax\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "# Experiment #1: How does normalized input data affect our model's learning outcome?\n",
    "\n",
    "In this experiment we are going to answer the above question by training identifical models on a normalized data set and on a non-normalized data set. \n",
    "\n",
    "Then we will \n",
    "\n",
    "- Analyze the trained weight values of our model \n",
    "- Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fit Model on Non-Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbe60638f98>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# fit model on non-normalized data\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = os.path.join(\"logs\", f\"No_Normalization-{now}\")\n",
    "tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
    "\n",
    "model = create_model(lr=.001, opt=\"adam\")\n",
    "\n",
    "model.fit(X_train, y_train, \n",
    "          validation_data=(X_test, y_test),\n",
    "          workers=-2, \n",
    "          epochs=10, \n",
    "          batch_size=32, \n",
    "          verbose=0, \n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------\n",
    "\n",
    "### Fit Model on Normalized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "ccc6ed9c1e2c04f77eac26d557236dad",
     "grade": false,
     "grade_id": "cell-2792e5f1fbf02c67",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Normalize your training and test sets \n",
    "# save normalized data to X_train_scaled and X_test_scaled\n",
    "\n",
    "# YOUR CODE HERE\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "#print(X_train[0], X_train_scaled[0], X_test[0], X_test_scaled[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7fbe80ba5f98>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model on normalized data\n",
    "\n",
    "now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "logdir = os.path.join(\"logs\", f\"Normalization-{now}\")\n",
    "tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
    "\n",
    "norm_model = create_model(lr=.001, opt=\"adam\")\n",
    "\n",
    "norm_model.fit(X_train_scaled, y_train, \n",
    "          validation_data=(X_test_scaled, y_test),\n",
    "          workers=-2, \n",
    "          epochs=10, \n",
    "          batch_size=32, \n",
    "          verbose=0, \n",
    "          callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Each layer is labeled\n",
    "\n",
    "Take note of the label for each layer in the network. It is these labels that will help you identify the corresponding bias and weight distribtuions on tensorboard. \n",
    "\n",
    "Assuming that you've ran `create_model` 3 times: once for the model check, once to create `model`, and once to create `norm_model`:\n",
    "\n",
    "The name of the layers for `model` should be \n",
    "- dense_2\n",
    "- dense_3\n",
    "\n",
    "The name of the layers for `norm_model` should be \n",
    "- dense_4\n",
    "- dense_5\n",
    "\n",
    "\n",
    "If you keep retraining one or both of these models, tensorflow will increment the integer used in the layer names.  But that doesn't really matter, just take notice of the layer names so you can find their corresponding bias and weight distribtuions in tensorboard.\n",
    "\n",
    "**Protip:** If you want to reset the integer incrementation that tensorflow uses, you'll need to restart your notebook's kernal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_2 (Dense)              (None, 250)               196250    \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 10)                2510      \n",
      "=================================================================\n",
      "Total params: 198,760\n",
      "Trainable params: 198,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_4 (Dense)              (None, 250)               196250    \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 10)                2510      \n",
      "=================================================================\n",
      "Total params: 198,760\n",
      "Trainable params: 198,760\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "norm_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard \n",
    "\n",
    "- Run the cell below to launch tensorboard \n",
    "- Click on the `SCALARS` tab to see plots that compare the loss and accuracy between the two models\n",
    "- Cick on the `HISTOGRAMS` tab to see the distribution of the learned weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-13c7dc685646e5d0\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-13c7dc685646e5d0\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8001;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs --port=8001 --host=localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensorboard \n",
    "\n",
    "Check out the loss and accuracy plots on the `SCALARS` tab. \n",
    "\n",
    "What you should see is that the accuracy is much higher for the model that was given normalized data; conversely, the loss is much lower for the model that was given normalized data. \n",
    "\n",
    "Recall that what we are doing whenever training a model is adjusting the value of the bias and weights in each layer. For simplicity of analysis, we only trained two layers: a hidden layer and the output layer. \n",
    "\n",
    "Now click on the `HISTOGRAM` tab. \n",
    "\n",
    "You should see both of your model's layer names. \n",
    "\n",
    "### Hidden Layer Distributions\n",
    "\n",
    "Collapse the charts that correspond to the output layer so only the distributions for the weights and bias in the hidden layer are showing. (i.e. Only expand `dense_2` and `dense_4`). \n",
    "\n",
    "Also don't be confused by the word `kernel`, that's just the word that Tensorflow uses instead of weights. So, to be clear, **the kernal distributions are the weight values.** \n",
    "\n",
    "The `bias` distributions are the bias values. \n",
    "\n",
    "You should see 10 distributions stacked next to each other, **one distribution per epoch.**\n",
    "\n",
    "The distribuion in the far back corresponds to the weight values at epoch 1 (tensorflow starts the count at 0, like the index for a list). The distribution at the very front corresponds to the weight values at the 10th epoch (tensorflow indexing show 9 instead of 10).\n",
    "\n",
    "Notice how the shape of the distribution changes accross epochs? That's because their **values are being updated via Gradient Descent.** \n",
    "\n",
    "The distributions that you see are direclty responsible for the validation accuracy of our models. The reason why they look different between the two models is because one model was given normalize data and one wasn't. So you can conclude that the weight distributions in `dense_4` produce a higher validation accuracy than the weight distributions in `dense_2`. \n",
    "\n",
    "Now it's time to analyze those weight values more closely. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------\n",
    "### Analyze Weights in Each Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final bias and weight matrices for model\n",
    "layer = model.get_layer(name=\"dense_2\")\n",
    "weights, bias = layer.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the final  value bias and weight magrices for norm_moel\n",
    "layer = norm_model.get_layer(name=\"dense_4\")\n",
    "weights_norm, bias_norm = layer.get_weights()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this line of code should not throw an error if the number of weights is the same for the hidden layer of both models\n",
    "# this line of code is known as a Unit Test \n",
    "assert weights.shape[0] == weights_norm.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Weight Values\n",
    "\n",
    "By default, Keras dense layers randomly initialize the weight values using [**GlorotUniform**](https://keras.io/api/layers/initializers/). \n",
    "\n",
    "The cell below is sampling values from the GlorotUniform distribution. Let's sample from the GlorotUniform distribution and plot it in order to get a sense of the initial distribution of our weights - before Gradient Descent starts upading their values at training time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 784), dtype=float32, numpy=\n",
       "array([[ 0.04225676,  0.08248918, -0.05188543,  0.05000354, -0.01401389,\n",
       "        -0.05038945, -0.00550017,  0.08530942,  0.08367802, -0.04946244,\n",
       "        -0.00390391,  0.01237956, -0.03458299,  0.05426465, -0.0144887 ,\n",
       "        -0.04014378,  0.02067915, -0.00902664,  0.05755775,  0.04539782,\n",
       "         0.07509173,  0.05108289,  0.03125129, -0.03421508,  0.07618853,\n",
       "         0.02212521,  0.01809362, -0.05210731,  0.03646112, -0.00515051,\n",
       "         0.06872776,  0.08025235, -0.0175729 ,  0.08241746,  0.00652631,\n",
       "        -0.02771863, -0.06154662,  0.04336341, -0.0255204 , -0.00239493,\n",
       "        -0.07664959, -0.01624235,  0.08614823,  0.00767895,  0.00591909,\n",
       "        -0.07532714, -0.00086845,  0.07434253, -0.003987  ,  0.0136476 ,\n",
       "         0.05740948, -0.03742301,  0.07635556,  0.02686686, -0.0366196 ,\n",
       "         0.06338313,  0.07979017,  0.03797166,  0.05389202, -0.03794813,\n",
       "        -0.00494084, -0.03403559, -0.02199777, -0.04592917, -0.05201837,\n",
       "        -0.04021104, -0.05694082,  0.08317132, -0.07027904, -0.07976422,\n",
       "        -0.04871043, -0.06844635,  0.07622917,  0.01469178,  0.02727044,\n",
       "        -0.01567397, -0.04864268,  0.06451975, -0.03402258,  0.02630645,\n",
       "        -0.06856489,  0.07046822,  0.02516011, -0.077855  , -0.04723025,\n",
       "         0.04698184,  0.07262576,  0.0124003 , -0.03754749,  0.0806836 ,\n",
       "         0.07054941,  0.02668108, -0.02672369, -0.02149905, -0.01659159,\n",
       "         0.06888345, -0.0329111 ,  0.02602414, -0.08530962,  0.02359334,\n",
       "        -0.06088242,  0.00734027,  0.03409683,  0.04526989, -0.0164695 ,\n",
       "        -0.0343209 , -0.02998227, -0.04206359, -0.00601416, -0.01135837,\n",
       "         0.01757903,  0.05045146,  0.00582225, -0.048719  , -0.0608132 ,\n",
       "        -0.03241847,  0.08597533,  0.08025242, -0.04656573, -0.02774268,\n",
       "        -0.08368675, -0.06949918,  0.05860049,  0.00427817,  0.04586038,\n",
       "        -0.00152903, -0.02092664, -0.04496001, -0.02911708,  0.00513594,\n",
       "        -0.07601101, -0.0030112 ,  0.01608543, -0.06808098,  0.07191367,\n",
       "         0.01167899,  0.07717884,  0.02645247, -0.00026634,  0.05836883,\n",
       "        -0.08230875,  0.05774455,  0.07177065,  0.04997635, -0.04609269,\n",
       "        -0.05410454, -0.0025785 ,  0.08515002, -0.03076454,  0.00972316,\n",
       "        -0.06174982,  0.00145824,  0.01209277,  0.06179264,  0.0420415 ,\n",
       "        -0.03204495,  0.03777726,  0.01449864,  0.02102098,  0.06107883,\n",
       "         0.00338529,  0.05257708, -0.06065735,  0.03855979, -0.0425607 ,\n",
       "        -0.07154632, -0.04043311,  0.0173343 ,  0.0440454 ,  0.01721393,\n",
       "        -0.04778016,  0.07885097,  0.00675214, -0.01762441, -0.01867396,\n",
       "        -0.01357599, -0.06867658, -0.04471797, -0.06338043,  0.0520439 ,\n",
       "        -0.01482018, -0.05090707,  0.08575121, -0.07981618,  0.0051294 ,\n",
       "         0.06894642, -0.06315316,  0.04401277,  0.02016285,  0.0182536 ,\n",
       "        -0.0660409 ,  0.08064082, -0.02643745,  0.00669565, -0.04556638,\n",
       "        -0.06973366, -0.00348662,  0.01722962,  0.0025141 ,  0.07175227,\n",
       "         0.05456327, -0.01556152, -0.0749405 ,  0.06645639, -0.01233701,\n",
       "         0.03010514, -0.04629811, -0.083072  , -0.00543013,  0.06141713,\n",
       "        -0.08728848,  0.0104779 ,  0.01669149,  0.04272521,  0.04804194,\n",
       "         0.00043699,  0.03879653, -0.02231795,  0.0535741 , -0.06933681,\n",
       "        -0.02664158,  0.06077124,  0.05935298, -0.08117498, -0.05160022,\n",
       "        -0.01477293, -0.00976543, -0.02666084, -0.02821711,  0.0054239 ,\n",
       "         0.04590651, -0.01234359, -0.00940227,  0.0075785 , -0.00021221,\n",
       "        -0.05218839,  0.0744518 , -0.07393476, -0.01117682, -0.05698362,\n",
       "        -0.00283149, -0.07186551, -0.03541375,  0.04792017,  0.01041897,\n",
       "        -0.02951549,  0.0009553 , -0.00547334,  0.03526536,  0.00817411,\n",
       "         0.08645162,  0.00770273,  0.06034848,  0.02801139,  0.05005738,\n",
       "         0.04488049,  0.01619209, -0.0146541 ,  0.05817375, -0.06536348,\n",
       "        -0.06395821, -0.0785703 , -0.05915204, -0.03184124,  0.0386523 ,\n",
       "        -0.04665652, -0.08316109,  0.0687013 , -0.00724197,  0.04601674,\n",
       "        -0.02839254, -0.02138709,  0.07058866,  0.04317798,  0.00664631,\n",
       "        -0.06885815,  0.0124794 ,  0.08491938, -0.08595401,  0.01385373,\n",
       "        -0.02824648, -0.0840707 , -0.00672769, -0.0649515 ,  0.06034869,\n",
       "         0.00164793, -0.00876473, -0.06077833,  0.05276158, -0.0320231 ,\n",
       "        -0.06503066, -0.04858649, -0.07350986, -0.02205394,  0.07125139,\n",
       "         0.07256965,  0.01691084,  0.04076883, -0.01964971,  0.02003901,\n",
       "         0.08380885,  0.02749687, -0.06621215,  0.02117664,  0.03229493,\n",
       "        -0.03104496, -0.05966659, -0.03475861,  0.02564123,  0.02599754,\n",
       "         0.01735183, -0.07193089, -0.0727507 ,  0.02770477,  0.0188423 ,\n",
       "        -0.01007205,  0.01149588,  0.03544141,  0.07608636, -0.0044341 ,\n",
       "         0.00937169,  0.06430915, -0.03112594, -0.05787714,  0.04380554,\n",
       "        -0.02416801, -0.07302897, -0.00927743, -0.04405236,  0.05135655,\n",
       "        -0.02781195,  0.02759442, -0.05894743, -0.05823119,  0.06592727,\n",
       "         0.07783633,  0.08412506,  0.06775671,  0.03754187, -0.03157561,\n",
       "        -0.00551128,  0.05850175, -0.03473107, -0.0216109 ,  0.02798683,\n",
       "         0.01139345,  0.08634505,  0.03427333, -0.00108966, -0.03411376,\n",
       "        -0.06386711, -0.05737503, -0.00279804,  0.07950446, -0.05458574,\n",
       "        -0.05895867,  0.0445365 ,  0.04412667,  0.06831302, -0.03365619,\n",
       "         0.05648115,  0.08574678, -0.01738472,  0.00093283,  0.05925344,\n",
       "        -0.02013969,  0.00128628,  0.04123771, -0.05781654, -0.02417275,\n",
       "         0.021645  , -0.05857109, -0.02610422, -0.04269321, -0.01440959,\n",
       "        -0.07416295,  0.02681588, -0.02727076, -0.03305034, -0.03628951,\n",
       "        -0.02487378,  0.06026612, -0.04347198, -0.07445453,  0.02394595,\n",
       "         0.02774975, -0.03756262,  0.00960741, -0.02547911, -0.08573513,\n",
       "         0.07042728,  0.04269798, -0.06732184, -0.02520064,  0.02381426,\n",
       "         0.03495056,  0.00047558, -0.01802611,  0.00520231, -0.08314416,\n",
       "         0.01321515,  0.05992496,  0.03435175,  0.04753166,  0.06102911,\n",
       "        -0.01338194,  0.05300714,  0.07318515,  0.04754521, -0.04651639,\n",
       "        -0.07264062,  0.08014974, -0.0144763 , -0.02179769, -0.03209785,\n",
       "        -0.07096241,  0.05246609, -0.04523157,  0.02336555,  0.03109352,\n",
       "        -0.03447767,  0.02591821, -0.04774008, -0.08088294,  0.05983075,\n",
       "        -0.03744479, -0.0409674 , -0.01544557, -0.02080128, -0.00587221,\n",
       "        -0.0216794 ,  0.04839751,  0.07510754,  0.05310069, -0.07368833,\n",
       "        -0.06712633,  0.00706695,  0.00801896, -0.05788693, -0.03252742,\n",
       "         0.0826128 , -0.07382815, -0.02234359,  0.05588064,  0.08715463,\n",
       "         0.01718351, -0.06097461,  0.05277644,  0.01539406,  0.04894969,\n",
       "        -0.07644844,  0.03690356,  0.07488097,  0.04022299, -0.04212191,\n",
       "        -0.07397851,  0.08660035,  0.07507277,  0.08250481, -0.05911275,\n",
       "        -0.00643594,  0.05434704,  0.06479307, -0.07701067, -0.0620954 ,\n",
       "         0.04428203, -0.02113266,  0.06520867,  0.06524007,  0.02719378,\n",
       "         0.00239035,  0.05035402, -0.07282142, -0.05488979,  0.05856369,\n",
       "        -0.01072717,  0.0641813 ,  0.04181607, -0.07865687,  0.00249163,\n",
       "        -0.06631147, -0.04289106, -0.07174383, -0.01304384, -0.07938803,\n",
       "         0.08359174,  0.05785304,  0.03081913,  0.08230495, -0.02348338,\n",
       "        -0.05967113,  0.07159577,  0.02820175, -0.03569179,  0.05979796,\n",
       "        -0.05618377,  0.03527921,  0.03427158,  0.03926168,  0.02508562,\n",
       "         0.04161188, -0.07046361,  0.05775364, -0.08715271,  0.04615943,\n",
       "        -0.03904128,  0.04154733,  0.07628381, -0.03228221,  0.0643979 ,\n",
       "        -0.02103338,  0.00343134, -0.06730346,  0.02016065,  0.08164346,\n",
       "        -0.00603194, -0.03841708, -0.01508121, -0.02464902, -0.04759892,\n",
       "        -0.06387706, -0.01590286, -0.01200674,  0.07084572, -0.00145881,\n",
       "         0.0347407 ,  0.05418393,  0.08136992, -0.05482191,  0.05823611,\n",
       "         0.07302044,  0.08211759,  0.05911586, -0.07481799, -0.07715183,\n",
       "         0.04475404,  0.04139566,  0.02496268, -0.02824886, -0.01238838,\n",
       "        -0.01588549, -0.00098736, -0.05322032,  0.00770772,  0.03709839,\n",
       "        -0.08413967, -0.04420206,  0.01084974, -0.04778054, -0.01175468,\n",
       "         0.03836092, -0.05554878,  0.06825197, -0.02920658, -0.08408748,\n",
       "        -0.01974082,  0.0277258 , -0.03689001, -0.04706671, -0.07815006,\n",
       "        -0.02560695, -0.01219825, -0.05032886,  0.048575  ,  0.06540702,\n",
       "         0.04622538, -0.00261971, -0.07842492,  0.08532394, -0.0354945 ,\n",
       "         0.02367146,  0.01363183, -0.01989119, -0.02189136, -0.05502001,\n",
       "        -0.03361823, -0.07329523, -0.07762951,  0.0712219 ,  0.02844969,\n",
       "         0.01024216, -0.05327336,  0.08549134, -0.06299283,  0.03601187,\n",
       "        -0.08033287, -0.01051671, -0.07837637, -0.08323931,  0.00442974,\n",
       "        -0.06730043,  0.00296085, -0.01789025,  0.05764492,  0.02962565,\n",
       "         0.01007036, -0.07269432,  0.02329995, -0.02806585, -0.05864839,\n",
       "        -0.08113694,  0.06159785,  0.07189222, -0.04480587,  0.04718156,\n",
       "         0.03550512,  0.08269583,  0.02180842,  0.01464853, -0.01573499,\n",
       "         0.00156115, -0.04311915,  0.0169792 , -0.02009381,  0.0405187 ,\n",
       "        -0.08347808,  0.08666694,  0.03144535, -0.05344783, -0.02098905,\n",
       "         0.07232589,  0.03937635,  0.0320798 , -0.07728173, -0.03360335,\n",
       "        -0.02822702, -0.07516183, -0.02902726, -0.08659545,  0.06025612,\n",
       "        -0.03182181, -0.06126633,  0.07610856, -0.03018008,  0.08370787,\n",
       "        -0.03384952, -0.0145525 , -0.00356718,  0.00975147, -0.00962223,\n",
       "        -0.0564898 , -0.00036836, -0.07688994, -0.06279496, -0.02943438,\n",
       "         0.0727966 ,  0.00468142,  0.05819601,  0.04437694,  0.0107688 ,\n",
       "        -0.03436653, -0.08409317, -0.08004309, -0.00563559, -0.08422065,\n",
       "         0.04118326,  0.08401208,  0.05532625, -0.0108424 ,  0.02646914,\n",
       "         0.00484738, -0.00341908, -0.03574052, -0.00766186,  0.03432669,\n",
       "        -0.05104233,  0.0085237 , -0.04434505, -0.08572281,  0.03946762,\n",
       "         0.03867324,  0.02511724,  0.05940436,  0.00622132,  0.05585065,\n",
       "        -0.02029765, -0.03114578,  0.07801905,  0.04838875, -0.08044591,\n",
       "        -0.01337826, -0.08612731, -0.05851206,  0.05119219, -0.02415863,\n",
       "         0.06912485,  0.02589168, -0.04250846,  0.06752047,  0.0233806 ,\n",
       "         0.07225315, -0.08309482, -0.02658009, -0.00080895, -0.05781988,\n",
       "         0.04353063, -0.05486443,  0.07026316, -0.06905971, -0.05416115,\n",
       "        -0.06045756,  0.04969537, -0.00107799,  0.0239106 , -0.0223417 ,\n",
       "        -0.06626421, -0.08673454, -0.00435171, -0.01958808,  0.07896434,\n",
       "        -0.05565235, -0.06744365,  0.02420021, -0.07593893,  0.02912018,\n",
       "        -0.07863364,  0.027216  , -0.02745183,  0.06320731, -0.0002109 ,\n",
       "        -0.07547642,  0.03888613,  0.08322076, -0.03164491, -0.08158686,\n",
       "         0.08177936,  0.08229882,  0.02768026,  0.00649101, -0.00491989,\n",
       "        -0.00082999,  0.075909  , -0.01626384,  0.061419  , -0.07093106,\n",
       "        -0.04197104,  0.03563364,  0.05990089,  0.04604892,  0.03450035,\n",
       "         0.02656711, -0.01401475, -0.03859605, -0.04311746, -0.02423806,\n",
       "         0.0644014 , -0.06408529, -0.01606332,  0.0129782 ,  0.07241423,\n",
       "         0.05102119,  0.012915  ,  0.02169601,  0.06193775,  0.02962321,\n",
       "         0.0622184 , -0.02471662,  0.02916435,  0.00395352, -0.02742375,\n",
       "         0.01584156, -0.03888003, -0.00399683,  0.05902891,  0.05856065,\n",
       "         0.0104419 , -0.0007605 ,  0.05129294,  0.07184184,  0.00934851,\n",
       "        -0.05554144,  0.03371757, -0.01319297, -0.07465309,  0.0793613 ,\n",
       "        -0.048851  ,  0.00275218, -0.08727282,  0.01333626, -0.04797411,\n",
       "         0.02797762, -0.01460897,  0.02306456,  0.01044947]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's take 784 random samples form the GlorotUniform\n",
    "# because they are random samples their exact values might have been a little different for model and norm_model - but we will assume that they were not statistically different \n",
    "# 784 because that's how many weights are in the hidden layer for both of our models\n",
    "initializer = tf.keras.initializers.GlorotUniform(seed=1234)\n",
    "initial_weight_values = initializer(shape=(1, 784))\n",
    "initial_weight_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize=(20,6))\n",
    "plt.title(\"Learned Weights in Hidden Layers\")\n",
    "plt.xlabel(\"Weight Values\")\n",
    "plt.grid()\n",
    "\n",
    "# by setting density=True, we are transforming our plots into probability distributions \n",
    "plt.hist(weights.T[0], bins=20, alpha=0.5, label=\"weights from model\", density=True);\n",
    "plt.hist(weights_norm.T[0], bins=20, alpha=0.5, label=\"weights from norm_model\", density=True);\n",
    "plt.hist(initial_weight_values, bins=20, alpha=0.5, label=\"initial weight values\", density=True);\n",
    "plt.legend(fontsize=\"x-large\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations\n",
    "\n",
    "Your plot should have 3 distributions\n",
    "\n",
    "- weights from model trained on non-normalized data\n",
    "- weights from model trained on normalized data\n",
    "- initial weight values sampled from a Glorot Uniform distributions \n",
    "\n",
    "Use the plot to answer the following questions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the initial weights with weights_from_model, what was the effect of not using normalized data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "500ff9c3902f635e0a9aafa64e2cc95d",
     "grade": true,
     "grade_id": "cell-4d6c92df9d105c46",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The weights are less uniformly distributed over a smaller range of values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Comparing the initial weights with weights_from_norm_model, what was the effect of using normalized data?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "23d7012c3d7ce759e5389a0dfee1892a",
     "grade": true,
     "grade_id": "cell-614992ba50bf54c4",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The weights are more uniformly distributed over a larger ranger of values "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using your understand of how Gradient Descent works, why do you think that the distributions between weights_from_model and weights_from_norm_model look so different?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "52e82163ac0a8d8b47bc613199650fe2",
     "grade": true,
     "grade_id": "cell-598a597c991950f8",
     "locked": false,
     "points": 0,
     "schema_version": 3,
     "solution": true,
     "task": false
    }
   },
   "source": [
    "The weights are adjusted based on the gradient value wrt to weight per input line. This gradient value is proportional to the data value i.e. the bigger the data value, the bigger the adjustment to the weight and therefore most of the weight changes will be in the lines with the larger values. Normalizing the input data results in the weight adjustment being spread out more uniformly among the input lines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----\n",
    "\n",
    "# Additional Experiments\n",
    "\n",
    "The previous experiment demonstrated the importance of normalizing our data in order to maximize model accuracy. In the next few experiments, we are going to explore the effect that certain values for Batch Size, Learning Rate, and different Optimizers have on model accuracy. \n",
    "\n",
    "Using our **create_model** model building function, conduct the following experiments. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "f0pCkh8C7eGL"
   },
   "source": [
    "### Experiment with Batch Size\n",
    "* Run 5 experiments with various batch sizes of your choice. \n",
    "* Visualize the results\n",
    "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against your model's performance yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "USXjs7Hk71Hy"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with batch_size=16\n",
      "Training with batch_size=32\n",
      "Training with batch_size=64\n",
      "Training with batch_size=128\n",
      "Training with batch_size=256\n",
      "Done with experiments\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_batch_size/\n",
    "\n",
    "def train_with_batch_size(batch_size):\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = os.path.join(\"logs_batch_size\", f\"BatchSize({batch_size})-{now}\")\n",
    "    tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
    "\n",
    "    norm_model = create_model(lr=.001, opt=\"adam\")\n",
    "    norm_model.fit(X_train_scaled, y_train, \n",
    "              validation_data=(X_test_scaled, y_test),\n",
    "              epochs=10, \n",
    "              batch_size=batch_size, \n",
    "              verbose=0, \n",
    "              callbacks=[tensorboard])\n",
    "    \n",
    "for batch_size in [16, 32, 64, 128, 256]:\n",
    "    print(f'Training with batch_size={batch_size}')\n",
    "    train_with_batch_size(batch_size)\n",
    "    \n",
    "print('Done with experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-c33740fafc877fb9\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-c33740fafc877fb9\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8002;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_batch_size --port=8002 --host=localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch sizes of 32 and 64 have similar validation_accuracy performance but a batch size of 32 provides better training_accuracy as well, so we chose 32 as the optimum batch size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8b-r70o8p2Dm"
   },
   "source": [
    "### Experiment with Learning Rate\n",
    "* Run 5 experiments with various learning rate magnitudes: 1, .1, .01, .001, .0001.\n",
    "* Use the \"best\" batch size from the previous experiment\n",
    "* Visualize the results\n",
    "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_SA144xx8Luf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with learning_rate=1\n",
      "Training with learning_rate=0.1\n",
      "Training with learning_rate=0.01\n",
      "Training with learning_rate=0.001\n",
      "Training with learning_rate=0.0001\n",
      "Done with experiments\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_learning_rate/\n",
    "\n",
    "def train_with_learning_rate(learning_rate):\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = os.path.join(\"logs_learning_rate\", f\"LearningRate({learning_rate})-{now}\")\n",
    "    tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
    "\n",
    "    norm_model = create_model(lr=learning_rate, opt=\"adam\")\n",
    "    norm_model.fit(X_train_scaled, y_train, \n",
    "              validation_data=(X_test_scaled, y_test),\n",
    "              epochs=10, \n",
    "              batch_size=32, \n",
    "              verbose=0, \n",
    "              callbacks=[tensorboard])\n",
    "    \n",
    "for learning_rate in [1, 0.1, 0.01, 0.001, 0.0001]:\n",
    "    print(f'Training with learning_rate={learning_rate}')\n",
    "    train_with_learning_rate(learning_rate)\n",
    "    \n",
    "print('Done with experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8003 (pid 23295), started 0:00:15 ago. (Use '!kill 23295' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-5a5e15afdd1ab68e\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-5a5e15afdd1ab68e\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8003;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_learning_rate --port=8003 --host=localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A learning rate of 0.001 provides the best performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gxMtSRhV9Q7I"
   },
   "source": [
    "### Experiment with different Optimizers\n",
    "* Run 5 experiments with various optimizers available in TensorFlow. See list [here](https://www.tensorflow.org/api_docs/python/tf/keras/optimizers)\n",
    "* Visualize the results\n",
    "* Write up an analysis of the experiments and select the \"best\" performing model among your experiments. Make sure to compare against the previous experiments and your model's performance yesterday.\n",
    "* Repeat the experiment combining Learning Rate and different optimizers. Does the best performing model change? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ujLuzdNA91ip"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with optimizer=adam\n",
      "Training with optimizer=sgd\n",
      "Training with optimizer=adamax\n",
      "Training with optimizer=adagrad\n",
      "Training with optimizer=adadelta\n",
      "Done with experiments\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_optimizer/\n",
    "\n",
    "def train_with_optimizer(optimizer):\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = os.path.join(\"logs_optimizer\", f\"Optimizer({optimizer})-{now}\")\n",
    "    tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
    "\n",
    "    norm_model = create_model(lr=0.001, opt=optimizer)\n",
    "    norm_model.fit(X_train_scaled, y_train, \n",
    "              validation_data=(X_test_scaled, y_test),\n",
    "              epochs=10, \n",
    "              batch_size=32, \n",
    "              verbose=0, \n",
    "              callbacks=[tensorboard])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for optimizer in [\"adam\", \"sgd\", \"adamax\", \"adagrad\", \"adadelta\"]:\n",
    "    print(f'Training with optimizer={optimizer}')\n",
    "    train_with_optimizer(optimizer)\n",
    "    \n",
    "print('Done with experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 8004 (pid 23301), started 0:00:00 ago. (Use '!kill 23301' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-9d516f57262ba6e4\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-9d516f57262ba6e4\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8004;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_optimizer --port=8004 --host=localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adam and Adamax are the best performing optimizers with Adam winning by a nose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with optimizer=adam learning_rate=1\n",
      "Training with optimizer=adam learning_rate=0.1\n",
      "Training with optimizer=adam learning_rate=0.01\n",
      "Training with optimizer=adam learning_rate=0.001\n",
      "Training with optimizer=adam learning_rate=0.0001\n",
      "Training with optimizer=sgd learning_rate=1\n",
      "Training with optimizer=sgd learning_rate=0.1\n",
      "Training with optimizer=sgd learning_rate=0.01\n",
      "Training with optimizer=sgd learning_rate=0.001\n",
      "Training with optimizer=sgd learning_rate=0.0001\n",
      "Training with optimizer=adamax learning_rate=1\n",
      "Training with optimizer=adamax learning_rate=0.1\n",
      "Training with optimizer=adamax learning_rate=0.01\n",
      "Training with optimizer=adamax learning_rate=0.001\n",
      "Training with optimizer=adamax learning_rate=0.0001\n",
      "Training with optimizer=adagrad learning_rate=1\n",
      "Training with optimizer=adagrad learning_rate=0.1\n",
      "Training with optimizer=adagrad learning_rate=0.01\n",
      "Training with optimizer=adagrad learning_rate=0.001\n",
      "Training with optimizer=adagrad learning_rate=0.0001\n",
      "Training with optimizer=adadelta learning_rate=1\n",
      "Training with optimizer=adadelta learning_rate=0.1\n",
      "Training with optimizer=adadelta learning_rate=0.01\n",
      "Training with optimizer=adadelta learning_rate=0.001\n",
      "Training with optimizer=adadelta learning_rate=0.0001\n",
      "Done with experiments\n"
     ]
    }
   ],
   "source": [
    "# Clear any logs from previous runs\n",
    "!rm -rf ./logs_optimizer_learning_rate/\n",
    "\n",
    "def train_with_optimizer_learning_rate(optimizer, learning_rate):\n",
    "    now = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "    logdir = os.path.join(\"logs_optimizer_learning_rate\", f\"OptimizerLearningRate({optimizer}:{learning_rate})-{now}\")\n",
    "    tensorboard = TensorBoard(log_dir=logdir,  histogram_freq=1)\n",
    "\n",
    "    norm_model = create_model(lr=learning_rate, opt=optimizer)\n",
    "    norm_model.fit(X_train_scaled, y_train, \n",
    "              validation_data=(X_test_scaled, y_test),\n",
    "              epochs=10, \n",
    "              batch_size=32, \n",
    "              verbose=0, \n",
    "              callbacks=[tensorboard])\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "for optimizer in [\"adam\", \"sgd\", \"adamax\", \"adagrad\", \"adadelta\"]:\n",
    "    for learning_rate in [1, 0.1, 0.01, 0.001, 0.0001]:\n",
    "        print(f'Training with optimizer={optimizer} learning_rate={learning_rate}')\n",
    "        train_with_optimizer_learning_rate(optimizer, learning_rate)\n",
    "\n",
    "print('Done with experiments')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e4579c152d4136f2\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e4579c152d4136f2\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 8005;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%tensorboard --logdir logs_optimizer_learning_rate --port=8005 --host=localhost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performers are Adam(0.001), SGD(1) and Adamax(0.01) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FwlRJSfBlCvy"
   },
   "source": [
    "------\n",
    "\n",
    "## Stretch Goals: \n",
    "\n",
    "- On the learning rate experiments, implement [EarlyStopping](https://keras.io/api/callbacks/early_stopping/)\n",
    "- Review the math of Gradient Descent. "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_432_Train_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "U4-S2-NN (Python3)",
   "language": "python",
   "name": "u4-s2-nn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "nteract": {
   "version": "0.22.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
